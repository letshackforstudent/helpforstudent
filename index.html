<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Bmit Programs</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <h2 style="color: beige;">Bmit college of Eng,Bangalore</h2>
    <h3 style="color: beige;">Making Student's Life Easier </h3>

    <button class="copy-button" data-value='import seaborn as sns&#10;import matplotlib.pyplot as plt&#10;from sklearn.datasets import fetch_california_housing&#10;import numpy as np&#10;&#10;data = fetch_california_housing(as_frame=True)&#10;housing_df = data.frame&#10;&#10;numerical_features = housing_df.select_dtypes(include=[np.number]).columns&#10;&#10;plt.figure(figsize=(15, 10))&#10;plt.suptitle("Histograms of Numerical Features", fontsize=16)&#10;for i, feature in enumerate(numerical_features, 1):&#10;    plt.subplot(3, 3, i)&#10;    sns.histplot(housing_df[feature], kde=True, bins=30, color="blue")&#10;    plt.title(f"Distribution of {feature}")&#10;plt.tight_layout()&#10;plt.show()&#10;&#10;plt.figure(figsize=(15, 10))&#10;plt.suptitle("Box Plots of Numerical Features", fontsize=16)&#10;for i, feature in enumerate(numerical_features, 1):&#10;    plt.subplot(3, 3, i)&#10;    sns.boxplot(x=housing_df[feature], color="orange")&#10;    plt.title(f"Box Plot of {feature}")&#10;plt.tight_layout()&#10;plt.show()&#10;&#10;print("Outliers Detection:")&#10;outliers_summary = {}&#10;for feature in numerical_features:&#10;    Q1 = housing_df[feature].quantile(0.25)&#10;    Q3 = housing_df[feature].quantile(0.75)&#10;    IQR = Q3 - Q1&#10;    lower_bound = Q1 - 1.5 * IQR&#10;    upper_bound = Q3 + 1.5 * IQR&#10;    outliers = housing_df[(housing_df[feature] < lower_bound) | (housing_df[feature] > upper_bound)]&#10;    outliers_summary[feature] = len(outliers)&#10;    print(f"{feature}: {len(outliers)} outliers")&#10;&#10;print("\nDataset Summary:")&#10;print(housing_df.describe())'>1</button>

    <button class="copy-button" data-value='import pandas as pd&#10;import seaborn as sns&#10;import matplotlib.pyplot as plt&#10;from sklearn.datasets import fetch_california_housing&#10;&#10;california_data = fetch_california_housing(as_frame=True)&#10;data = california_data.frame&#10;&#10;correlation_matrix = data.corr()&#10;&#10;plt.figure(figsize=(10, 8))&#10;sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)&#10;plt.title("Correlation Matrix of California Housing Features")&#10;plt.show()&#10;&#10;sns.pairplot(data, diag_kind="kde", plot_kws={"alpha": 0.5})&#10;plt.suptitle("Pair Plot of California Housing Features", y=1.02)&#10;plt.show()'>2</button>

    <button class="copy-button" data-value='import numpy as np&#10;import pandas as pd&#10;from sklearn.datasets import load_iris&#10;from sklearn.decomposition import PCA&#10;import matplotlib.pyplot as plt&#10;&#10;iris = load_iris()&#10;data = iris.data&#10;labels = iris.target&#10;label_names = iris.target_names&#10;&#10;iris_df = pd.DataFrame(data, columns=iris.feature_names)&#10;&#10;pca = PCA(n_components=2)&#10;data_reduced = pca.fit_transform(data)&#10;&#10;reduced_df = pd.DataFrame(data_reduced, columns=["Principal Component 1", "Principal Component 2"])&#10;reduced_df["Label"] = labels&#10;&#10;plt.figure(figsize=(8, 6))&#10;colors = ["r", "g", "b"]&#10;for i, label in enumerate(np.unique(labels))::&#10;    plt.scatter(&#10;        reduced_df[reduced_df["Label"] == label]["Principal Component 1"],&#10;        reduced_df[reduced_df["Label"] == label]["Principal Component 2"],&#10;        label=label_names[label],&#10;        color=colors[i]&#10;    )&#10;plt.title("Scatter Plot - PCA on Iris Dataset")&#10;plt.xlabel("Principal Component 1")&#10;plt.ylabel("Principal Component 2")&#10;plt.legend()&#10;plt.grid()&#10;plt.show()&#10;&#10;explained_variance = pca.explained_variance_ratio_&#10;print("Explained Variance by each Principal Component:")&#10;print("Principal Component 1: ", explained_variance[0])&#10;print("Principal Component 2: ", explained_variance[1])&#10;print("Total Variance Retained: ", sum(explained_variance))'>3</button>

    <button class="copy-button" data-value='import csv&#10;&#10;num_attributes = 6&#10;a = []&#10;&#10;print("The Given Training Data Set")&#10;with open("enjoysport.csv", "r") as csvfile:&#10;    reader = csv.reader(csvfile)&#10;    for row in reader:&#10;        a.append(row)&#10;        print(row)&#10;&#10;print("The initial value of hypothesis:")&#10;hypothesis = ["0"] * num_attributes&#10;print(hypothesis)&#10;&#10;for j in range(num_attributes):&#10;    hypothesis[j] = a[0][j]&#10;&#10;print("Find S: Finding a Maximally Specific Hypothesis")&#10;for i in range(len(a))::&#10;    if a[i][num_attributes] == "yes":&#10;        for j in range(num_attributes):&#10;            if a[i][j] != hypothesis[j]:&#10;                hypothesis[j] = "?"&#10;            else:&#10;                hypothesis[j] = a[i][j]&#10;    print(f"For Training instance No:{i}, the hypothesis is", hypothesis)&#10;&#10;print("The Maximally Specific Hypothesis for the given Training Examples:")&#10;print(hypothesis)'>4</button>

    <button class="copy-button" data-value="import numpy as np
import matplotlib.pyplot as plt
from collections import Counter

n_samples = 100
np.random.seed(42)
x_values = np.random.rand(n_samples)

labels = np.where(x_values[:50] <= 0.5, 'Class1', 'Class2')

print('Generated x_values:')
print(x_values)
print('\nAssigned labels for first 50 points:')
print(labels)

def knn_classify(x_train, y_train, x_test, k):
    predictions = []
    for x in x_test:
        distances = np.abs(x_train - x)
        k_nearest_labels = y_train[np.argsort(distances)[:k]]
        predictions.append(Counter(k_nearest_labels).most_common(1)[0][0])
    return np.array(predictions)

k_values = [1, 2, 3, 4, 5, 20, 30]
results = {k: knn_classify(x_values[:50], labels, x_values[50:], k) for k in k_values}

for k, preds in results.items():
    print(f'Results for k={k}:')
    print(preds)

plt.figure(figsize=(10, 6))
for k in k_values:
    plt.scatter(x_values[50:], [k] * 50, c=['blue' if lbl == 'Class1' else 'red' for lbl in results[k]], label=f'k={k}', marker='o')

plt.xlabel('x values')
plt.ylabel('k values (for visualization)')
plt.title('KNN Classification Results for Different k Values')
plt.colorbar(label='Predicted Class')
plt.legend()
plt.show()">5</button>

<button class="copy-button" data-value='import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

data = fetch_openml(name="boston", version=1, as_frame=True)
X = data.data[["RM"]].values
y = data.target.astype(float).values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

scaler = StandardScaler()
X_train, X_test = scaler.fit_transform(X_train), scaler.transform(X_test)

def locally_weighted_regression(X, y, x_query, tau):
    m = X.shape[0]
    X_bias = np.c_[np.ones(m), X]
    x_query_bias = np.r_[1, x_query]
    
    W = np.diag(np.exp(-np.square(X - x_query) / (2 * tau**2)).flatten())
    
    theta = np.linalg.pinv(X_bias.T @ W @ X_bias) @ (X_bias.T @ W @ y)
    
    return x_query_bias @ theta

tau = 0.4
y_pred = np.array([locally_weighted_regression(X_train, y_train, x_q, tau) for x_q in X_test])

plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(X_train, y_train, label="Training Data", color="blue", alpha=0.5)
plt.scatter(X_test, y_test, label="Test Data", color="green", alpha=0.5)
plt.scatter(X_test, y_pred, color="red", label="LWR Predictions", alpha=0.7)
plt.xlabel("Average Rooms per Dwelling (Normalized)")
plt.ylabel("House Price")
plt.title("Locally Weighted Regression on Boston Housing Data")
plt.legend()

plt.subplot(1, 2, 2)
plt.scatter(y_test, y_pred, color="blue", alpha=0.7)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color="red", linestyle="--", label="Ideal Fit")
plt.xlabel("Actual House Price")
plt.ylabel("Predicted House Price")
plt.title("Prediction vs Actual Plot for LWR")
plt.legend()

plt.tight_layout()
plt.show()'>6</button>


    <button class="copy-button" data-value='import numpy as np&#10;import pandas as pd&#10;import matplotlib.pyplot as plt&#10;import seaborn as sns&#10;import warnings&#10;from sklearn.datasets import fetch_openml&#10;from sklearn.linear_model import LinearRegression&#10;from sklearn.preprocessing import PolynomialFeatures&#10;from sklearn.metrics import mean_squared_error, r2_score&#10;from sklearn.model_selection import train_test_split&#10;warnings.filterwarnings("ignore")&#10;boston = fetch_openml(name="boston", version=1, as_frame=True)&#10;X_boston = boston.data&#10;y_boston = boston.target&#10;X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_boston, y_boston, test_size=0.2, random_state=42)&#10;linear_model = LinearRegression()&#10;linear_model.fit(X_train_b, y_train_b)&#10;y_pred_b = linear_model.predict(X_test_b)&#10;print("----- Linear Regression: Boston Housing -----")&#10;print(f"MSE:{mean_squared_error(y_test_b, y_pred_b):.2f}")&#10;print(f"RMSE: {np.sqrt(mean_squared_error(y_test_b, y_pred_b)):.2f}")&#10;print(f"RÂ² Score: {r2_score(y_test_b, y_pred_b):.2f}")&#10;print()&#10;df = sns.load_dataset(&apos;mpg&apos;)&#10;df = df.dropna()&#10;df[&apos;horsepower&apos;] = df[&apos;horsepower&apos;].fillna(df[&apos;horsepower&apos;].median())&#10;X_mpg = df[[&apos;horsepower&apos;]]&#10;y_mpg = df[&apos;mpg&apos;]&#10;X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_mpg, y_mpg, test_size=0.2, random_state=42)&#10;poly = PolynomialFeatures(degree=2)&#10;X_train_poly = poly.fit_transform(X_train_m)&#10;X_test_poly = poly.transform(X_test_m)&#10;poly_model = LinearRegression()&#10;poly_model.fit(X_train_poly, y_train_m)&#10;y_pred_poly = poly_model.predict(X_test_poly)&#10;print("----- Polynomial Regression: Auto MPG (Degree 2) -----")&#10;print(f"MSE:{mean_squared_error(y_test_m, y_pred_poly):.2f}")&#10;print(f"RMSE: {np.sqrt(mean_squared_error(y_test_m, y_pred_poly)):.2f}")&#10;print(f"RÂ² Score: {r2_score(y_test_m, y_pred_poly):.2f}")&#10;print()&#10;X_range = np.linspace(X_mpg.min(), X_mpg.max(), 100).reshape(-1, 1)&#10;X_range_poly = poly.transform(X_range)&#10;y_range_pred = poly_model.predict(X_range_poly)&#10;plt.scatter(X_mpg, y_mpg, color=&apos;green&apos;, label=&apos;Actual Data&apos;)&#10;plt.plot(X_range, y_range_pred, color=&apos;red&apos;, label=&apos;Polynomial Fit (Degree 2)&apos;, linewidth=2)&#10;plt.xlabel("Horsepower")&#10;plt.ylabel("MPG")&#10;plt.title("Polynomial Regression: Horsepower vs MPG")&#10;plt.legend()&#10;plt.grid(True)&#10;plt.show()'>7</button>

    <button class="copy-button" data-value='import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

data = load_breast_cancer()
X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = DecisionTreeClassifier(criterion="gini", random_state=42)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy on test data: {accuracy*100:.2f}%")

new_sample = X_test[0].reshape(1, -1)
predicted_class = clf.predict(new_sample)[0]

print("Predicted class for the new sample:", data.target_names[predicted_class])
print("Actual class:", data.target_names[y_test[0]])

plt.figure(figsize=(20, 10))
plot_tree(clf, filled=True, feature_names=data.feature_names, class_names=data.target_names)
plt.title("Decision Tree for Breast Cancer Classification")
plt.show()'>8</button>

    <button class="copy-button" data-value='from sklearn.datasets import fetch_olivetti_faces
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt

faces = fetch_olivetti_faces()
X = faces.data
y = faces.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)

pca = PCA(n_components=100, whiten=True, random_state=42)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

gnb = GaussianNB()
gnb.fit(X_train_pca, y_train)

y_pred = gnb.predict(X_test_pca)
accuracy = accuracy_score(y_test, y_pred)

print("Accuracy of Naive Bayes on Olivetti Faces: {:.2f}%".format(accuracy * 100))
print("\nClassification Report:\n", classification_report(y_test, y_pred))'>9</button>

    <button class="copy-button" data-value='import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

data = load_breast_cancer()
X = StandardScaler().fit_transform(data.data)
y = data.target

kmeans = KMeans(n_clusters=2, random_state=42)
labels = kmeans.fit_predict(X)

print(f"Accuracy of K-Means clustering: {accuracy_score(y, labels) * 100:.2f}%")
print("\nConfusion Matrix:\n", confusion_matrix(y, labels))
print("\nClassification Report:\n", classification_report(y, labels))

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
centroids_pca = pca.transform(kmeans.cluster_centers_)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))
titles = ["True Labels", "K-Means Clustering"]

for ax, data_labels, title in zip(axes, [y, labels], titles):
    ax.scatter(X_pca[:, 0], X_pca[:, 1], c=data_labels, cmap="coolwarm", alpha=0.7)
    ax.set(title=title, xlabel="PCA Component 1", ylabel="PCA Component 2")
    ax.grid(True)

axes[1].scatter(*centroids_pca.T, marker="X", s=200, c="black", label="Centroids")
axes[1].legend()

plt.tight_layout()
plt.show()'>10</button>

  <!--  <button class="copy-button" data-value='import cv2&#10;image = cv2.imread("C:/Users/cse/Desktop/CG LAB/single.jpg")&#10;gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)&#10;_, thresh = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)&#10;contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)&#10;contour_image = image.copy()&#10;cv2.drawContours(contour_image, contours, -1, (0, 255, 0), 2)&#10;cv2.imshow("Image with Contours", contour_image)&#10;cv2.waitKey(0)&#10;cv2.destroyAllWindows()'>11</button>

    <button class="copy-button" data-value='import cv2&#10;face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")&#10;image = cv2.imread("C:/Users/cse/Desktop/CG LAB/group.jpg")&#10;gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)&#10;faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=6, minSize=(30, 30))&#10;for (x, y, w, h) in faces:&#10;    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)&#10;cv2.imshow("Faces Detected", image)&#10;cv2.waitKey(0)&#10;cv2.destroyAllWindows()'>12</button>
-->

    <script src="script.js"></script>
</body>
</html>
